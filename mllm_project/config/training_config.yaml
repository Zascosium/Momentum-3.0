# Training Configuration for Multimodal LLM
# Production-ready training parameters optimized for Databricks ML Runtime

# Training Parameters
training:
  epochs: 50
  batch_size: 16  # Adjust based on GPU memory
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  warmup_steps: 1000
  save_steps: 500
  eval_steps: 100
  logging_steps: 50
  
  # Early Stopping
  early_stopping:
    patience: 5
    min_delta: 0.001
    metric: "val_loss"
    mode: "min"

# Optimization
optimizer:
  name: "adamw"
  learning_rate: 5e-5
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8

# Learning Rate Scheduler
scheduler:
  name: "cosine_with_warmup"
  warmup_ratio: 0.1
  num_cycles: 0.5
  last_epoch: -1

# Mixed Precision Training
mixed_precision:
  enabled: true
  fp16: false
  bf16: true  # Better for stability on modern hardware
  loss_scale: 0  # Dynamic scaling (only used with fp16)
  init_scale: 65536  # Initial scale for fp16

# Distributed Training
distributed:
  backend: "nccl"
  find_unused_parameters: false
  gradient_as_bucket_view: true
  broadcast_buffers: false

# Checkpointing
checkpointing:
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
  save_last: true
  dirpath: "${CHECKPOINT_DIR:/dbfs/FileStore/mllm/checkpoints}"  # Use env var with fallback
  filename: "epoch_{epoch:02d}-val_loss_{val_loss:.2f}"
  auto_insert_metric_name: false  # Prevent duplicate metric names

# Validation
validation:
  val_check_interval: 0.25  # Check 4 times per epoch
  limit_val_batches: 1.0
  num_sanity_val_steps: 2

# Reproducibility
seed: 42
deterministic: true
benchmark: false

# Logging
logging:
  level: "INFO"
  log_every_n_steps: 50
  log_model_summary: true
  log_hyperparameters: true

# Memory Optimization
memory:
  pin_memory: true
  non_blocking: true
  persistent_workers: true
  prefetch_factor: 2
  max_memory_usage: 0.8  # Use up to 80% of available memory
  
# Environment Settings
environment:
  auto_detect_databricks: true
  setup_logging: true
  validate_setup: true