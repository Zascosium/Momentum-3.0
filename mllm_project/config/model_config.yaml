# Model Architecture Configuration for Multimodal LLM
# Production-ready model specifications

# Overall Model Configuration
model:
  name: "multimodal_llm"
  version: "1.0.0"
  description: "Time series + Text multimodal large language model"

# Time Series Encoder (MOMENT)
time_series_encoder:
  model_name: "AutonLab/MOMENT-1-large"
  embedding_dim: 512
  max_sequence_length: 512
  freeze_encoder: false  # Set to true for faster training
  dropout: 0.1
  
  # MOMENT specific parameters
  moment_config:
    n_channels: 1  # Will be adjusted based on data
    mask_ratio: 0.3
    patch_len: 8
    stride: 8
    normalize: true
    use_revin: true

# Text Decoder Configuration
text_decoder:
  model_name: "gpt2-medium"  # 345M parameters
  vocab_size: 50257
  embedding_dim: 1024
  num_layers: 24
  num_heads: 16
  intermediate_size: 4096
  max_position_embeddings: 1024
  dropout: 0.1
  attention_dropout: 0.1
  freeze_decoder: false
  
  # Generation parameters
  generation:
    max_length: 512
    min_length: 10
    do_sample: true
    temperature: 0.8
    top_k: 50
    top_p: 0.9
    repetition_penalty: 1.1
    length_penalty: 1.0
    early_stopping: true

# Projection Layers
projection:
  input_dim: 512  # MOMENT embedding dimension
  output_dim: 1024  # GPT-2 medium embedding dimension
  hidden_dims: [768, 1024]
  activation: "gelu"
  dropout: 0.1
  layer_norm: true
  residual_connection: true

# Cross-Attention Configuration
cross_attention:
  num_heads: 8
  hidden_size: 512
  intermediate_size: 2048
  attention_dropout: 0.1
  hidden_dropout: 0.1
  layer_norm_eps: 1e-12
  max_position_embeddings: 1024
  
  # Attention mechanism settings
  attention_type: "scaled_dot_product"
  use_relative_position: false
  attention_window: null  # Full attention
  
# Fusion Strategy
fusion:
  strategy: "cross_attention"  # Options: early, late, cross_attention, hybrid
  fusion_layers: [6, 12, 18]  # Which decoder layers to apply fusion
  gate_mechanism: true  # Learnable gating for fusion
  temperature: 1.0

# Special Tokens
special_tokens:
  time_series_start: "<|ts_start|>"
  time_series_end: "<|ts_end|>"
  text_start: "<|text_start|>"
  text_end: "<|text_end|>"
  pad_token: "<|pad|>"
  unk_token: "<|unk|>"
  
# Model Initialization
initialization:
  type: "normal"
  mean: 0.0
  std: 0.02
  bias_init: 0.0

# Regularization
regularization:
  weight_decay: 0.01
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.0
  layer_norm_eps: 1e-12

# Loss Configuration
loss:
  text_generation_weight: 1.0
  time_series_reconstruction_weight: 0.5
  alignment_loss_weight: 0.1
  label_smoothing: 0.1

# Model Size Constraints
constraints:
  max_parameters: "2B"  # Maximum model size
  max_memory_gb: 32  # Maximum GPU memory usage
  inference_max_tokens: 512